# ai_sbert_matcher.py
import json
import logging
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

# Potrzebne biblioteki - zainstaluj przez: pip install sentence-transformers numpy scikit-learn
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import config_manager

# Konfiguracja loggera
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rozszerzenia obrazÃ³w
IMAGE_EXTENSIONS = (
    ".jpg",
    ".jpeg",
    ".jpe",
    ".jfif",
    ".png",
    ".apng",
    ".gif",
    ".bmp",
    ".dib",
    ".webp",
    ".tiff",
    ".tif",
    ".svg",
    ".svgz",
    ".ico",
    ".avif",
    ".heic",
    ".heif",
)


class SBERTFileMatcher:
    """
    Klasa do dopasowywania nazw plikÃ³w uÅ¼ywajÄ…c Sentence-BERT
    """

    def __init__(self, model_name: str = "all-MiniLM-L6-v2", auto_optimize: bool = True):
        """
        Inicjalizacja z automatycznÄ… optymalizacjÄ… sprzÄ™towÄ…
        
        Args:
            model_name: Nazwa modelu SBERT
            auto_optimize: Czy automatycznie optymalizowaÄ‡ dla dostÄ™pnego sprzÄ™tu
        """
        self.hardware_config = None
        
        if auto_optimize:
            try:
                from hardware_detector import get_hardware_detector
                detector = get_hardware_detector()
                self.hardware_config = detector.optimal_config
                
                logger.info("ðŸ” Wykryte sprzÄ™t:")
                logger.info(detector.get_hardware_summary())
                
            except ImportError as e:
                logger.warning(f"ModuÅ‚ hardware_detector niedostÄ™pny: {e}")
                auto_optimize = False
            except Exception as e:
                logger.warning(f"BÅ‚Ä…d automatycznej optymalizacji: {e}")
                auto_optimize = False
        
        # Ustaw zmienne Å›rodowiskowe dla optymalizacji CPU
        if auto_optimize and self.hardware_config:
            self._setup_cpu_optimization()
        
        logger.info(f"Åadowanie modelu SBERT: {model_name}")
        start_time = time.time()

        try:
            # Konfiguracja device i model
            device = self._get_optimal_device() if auto_optimize else None
            
            if device:
                logger.info(f"ðŸš€ UÅ¼ywam urzÄ…dzenia: {device}")
                self.model = SentenceTransformer(model_name, device=device)
            else:
                self.model = SentenceTransformer(model_name)
            
            # Optymalizacje modelu
            if auto_optimize and self.hardware_config:
                self._apply_model_optimizations()
            
            load_time = time.time() - start_time
            logger.info(f"Model zaÅ‚adowany w {load_time:.2f}s")
            
        except Exception as e:
            logger.error(f"BÅ‚Ä…d Å‚adowania modelu: {e}")
            logger.info("PrÃ³bujÄ™ zaÅ‚adowaÄ‡ model bez optymalizacji...")
            self.model = SentenceTransformer(model_name)

        # Zachowaj oryginalne progi
        self.similarity_threshold = 0.30
        self.high_confidence_threshold = 0.60
        self.very_high_confidence_threshold = 0.80
        self.low_similarity_threshold = 0.20

    def _setup_cpu_optimization(self):
        """Konfiguruje optymalizacje CPU"""
        if not self.hardware_config:
            return
        
        # Ustaw liczbÄ™ wÄ…tkÃ³w
        num_threads = str(self.hardware_config.get('num_threads', 4))
        os.environ['OMP_NUM_THREADS'] = num_threads
        os.environ['MKL_NUM_THREADS'] = num_threads
        os.environ['NUMEXPR_NUM_THREADS'] = num_threads
        
        # Optymalizacje Intel MKL
        if self.hardware_config.get('optimization_level') in ['optimized_cpu', 'basic_cpu']:
            os.environ['MKL_ENABLE_INSTRUCTIONS'] = 'AVX2' if self.hardware_config.get('use_avx_optimization') else 'SSE4_2'
        
        logger.info(f"ðŸ”§ CPU zoptymalizowany: {num_threads} wÄ…tkÃ³w")

    def _get_optimal_device(self) -> Optional[str]:
        """Zwraca optymalne urzÄ…dzenie dla modelu"""
        if not self.hardware_config:
            return None
        
        device_type = self.hardware_config.get('device', 'cpu')
        
        if device_type == 'cuda':
            try:
                import torch
                if torch.cuda.is_available():
                    return 'cuda'
            except ImportError:
                logger.warning("PyTorch/CUDA niedostÄ™pne")
        
        elif device_type == 'mps':
            try:
                import torch
                if torch.backends.mps.is_available():
                    return 'mps'
            except ImportError:
                logger.warning("PyTorch/MPS niedostÄ™pne")
        
        return 'cpu'

    def _apply_model_optimizations(self):
        """Stosuje optymalizacje modelu"""
        if not self.hardware_config:
            return
        
        try:
            # Optymalizacja precyzji dla GPU
            if self.hardware_config.get('device') in ['cuda', 'mps'] and self.hardware_config.get('model_precision') == 'float16':
                self.model = self.model.half()
                logger.info("ðŸ”§ Model przeÅ‚Ä…czony na float16")
            
            # Kompilacja modelu dla PyTorch 2.0+
            if hasattr(self.model, '_modules'):
                try:
                    import torch
                    if hasattr(torch, 'compile') and torch.__version__ >= '2.0':
                        self.model = torch.compile(self.model, mode='reduce-overhead')
                        logger.info("ðŸš€ Model skompilowany z PyTorch 2.0")
                except Exception as e:
                    logger.debug(f"Kompilacja modelu nieudana: {e}")
            
        except Exception as e:
            logger.warning(f"BÅ‚Ä…d podczas optymalizacji modelu: {e}")

    def preprocess_filename(self, filename: str) -> str:
        """
        Ulepszone przetwarzanie z zachowaniem wiÄ™cej kontekstu
        """
        # UsuÅ„ rozszerzenie
        name_without_ext = os.path.splitext(filename)[0]
        
        # Zachowaj wiÄ™cej informacji - zamieÅ„ tylko podkreÅ›lenia i myÅ›lniki
        processed = re.sub(r"[_\-]", " ", name_without_ext)
        
        # Zachowaj kropki jako separatory dla numerÃ³w/wersji
        processed = re.sub(r"\.(?=\d)", " ", processed)  # Kropka przed cyfrÄ… -> spacja
        processed = re.sub(r"(?<=\d)\.(?=\d)", " ", processed)  # Kropka miÄ™dzy cyframi -> spacja
        
        # UsuÅ„ wielokrotne spacje
        processed = re.sub(r"\s+", " ", processed).strip()
        
        logger.debug(f"Preprocessing: '{filename}' -> '{processed}'")
        return processed

    def simple_string_similarity(self, str1: str, str2: str) -> float:
        """
        Prosta miara podobieÅ„stwa z priorytetem dla identycznych czÄ™Å›ci
        """
        # UsuÅ„ rozszerzenia i normalizuj
        clean1 = os.path.splitext(str1)[0].lower()
        clean2 = os.path.splitext(str2)[0].lower()
        
        # SprawdÅº dokÅ‚adne dopasowanie po normalizacji separatorÃ³w
        normalized1 = re.sub(r"[_\-\.]", "", clean1)
        normalized2 = re.sub(r"[_\-\.]", "", clean2)
        
        if normalized1 == normalized2:
            return 1.0  # Identyczne po normalizacji = 100% dopasowania
        
        # Reszta logiki jak wczeÅ›niej...
        words1 = set(self.preprocess_filename(str1).lower().split())
        words2 = set(self.preprocess_filename(str2).lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0

    def calculate_embeddings(self, filenames: List[str]) -> np.ndarray:
        """
        Oblicza embeddings z optymalizacjami sprzÄ™towymi
        """
        if not filenames:
            return np.array([])

        processed_names = [self.preprocess_filename(name) for name in filenames]
        
        logger.debug(f"Obliczanie embeddings dla {len(processed_names)} plikÃ³w")
        start_time = time.time()

        # UÅ¼yj optymalnego batch_size
        batch_size = self.hardware_config.get('batch_size', 32) if self.hardware_config else 32
        
        try:
            # Oblicz embeddings z optymalnym batch_size
            embeddings = self.model.encode(
                processed_names, 
                batch_size=batch_size,
                show_progress_bar=len(processed_names) > 50,
                convert_to_numpy=True,
                normalize_embeddings=True  # Normalizacja dla lepszej wydajnoÅ›ci cosine similarity
            )
            
            calc_time = time.time() - start_time
            performance_info = f"Embeddings obliczone w {calc_time:.2f}s"
            
            if self.hardware_config:
                items_per_second = len(processed_names) / calc_time if calc_time > 0 else 0
                performance_info += f" ({items_per_second:.1f} plikÃ³w/s, batch_size: {batch_size})"
            
            logger.debug(performance_info)
            
            return embeddings
            
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas obliczania embeddings: {e}")
            # Fallback bez optymalizacji
            return self.model.encode(processed_names, show_progress_bar=False)

    def find_best_matches(self, archive_files: List[str], image_files: List[str]) -> List[Dict]:
        """
        Znajduje najlepsze dopasowania miÄ™dzy plikami archiwum a obrazami
        """
        matches = []
        used_images = set()

        for archive_file in archive_files:
            best_similarity = 0.0
            best_image_idx = -1
            best_method = "NO_MATCH"

            # Pierwsza faza: SBERT
            try:
                embeddings = self.calculate_embeddings([archive_file] + image_files)
                if len(embeddings) > 1:
                    archive_embedding = embeddings[0]
                    image_embeddings = embeddings[1:]
                    
                    similarities = cosine_similarity([archive_embedding], image_embeddings)[0]
                    
                    for j, similarity in enumerate(similarities):
                        if j in used_images:
                            continue
                            
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_image_idx = j
                            best_method = "SBERT"
            except Exception as e:
                logger.warning(f"BÅ‚Ä…d SBERT dla {archive_file}: {e}")

            # Druga faza: proste dopasowanie
            if best_image_idx == -1:
                for j, image_file in enumerate(image_files):
                    if j in used_images:
                        continue
                    
                    simple_similarity = self.simple_string_similarity(archive_file, image_file)
                    if simple_similarity > best_similarity and simple_similarity >= self.low_similarity_threshold:
                        best_similarity = simple_similarity
                        best_image_idx = j
                        best_method = "SIMPLE_MATCH"

            # Trzecia faza: bardzo proste dopasowania dla pozostaÅ‚ych plikÃ³w
            if best_image_idx == -1:
                logger.debug(f"PrÃ³bujÄ™ bardzo proste dopasowanie dla '{archive_file}'")
                
                for j, image_file in enumerate(image_files):
                    if j in used_images:
                        continue
                    
                    # Bardzo proste dopasowanie - tylko nazwy bez rozszerzeÅ„
                    archive_base = os.path.splitext(archive_file)[0].lower()
                    image_base = os.path.splitext(image_file)[0].lower()
                    
                    # UsuÅ„ wszystkie separatory i porÃ³wnaj
                    archive_clean = re.sub(r'[_\-\.\s]', '', archive_base)
                    image_clean = re.sub(r'[_\-\.\s]', '', image_base)
                    
                    # SprawdÅº czy jedna nazwa zawiera drugÄ…
                    if (archive_clean in image_clean or image_clean in archive_clean) and len(archive_clean) > 2:
                        very_simple_similarity = 0.25  # Przypisz staÅ‚y wynik dla tego typu dopasowania
                        if very_simple_similarity > best_similarity:
                            best_similarity = very_simple_similarity
                            best_image_idx = j
                            best_method = "VERY_SIMPLE_MATCH"

            if best_image_idx != -1:
                used_images.add(best_image_idx)
                matches.append({
                    "archive_file": archive_file,
                    "image_file": image_files[best_image_idx],
                    "similarity": float(best_similarity),
                    "method": best_method
                })

        return matches

    def debug_matching_process(self, archive_file: str, image_files: List[str]) -> Dict:
        """
        Funkcja debugowania pokazujÄ…ca dlaczego dopasowania mogÄ… nie dziaÅ‚aÄ‡
        """
        debug_info = {
            "archive_file": archive_file,
            "processed_archive": self.preprocess_filename(archive_file),
            "candidates": []
        }
        
        archive_embedding = self.calculate_embeddings([archive_file])
        image_embeddings = self.calculate_embeddings(image_files)
        
        if len(archive_embedding) > 0 and len(image_embeddings) > 0:
            similarities = cosine_similarity(archive_embedding, image_embeddings)[0]
            
            for i, image_file in enumerate(image_files):
                sbert_sim = similarities[i]
                simple_sim = self.simple_string_similarity(archive_file, image_file)
                
                candidate_info = {
                    "image_file": image_file,
                    "processed_image": self.preprocess_filename(image_file),
                    "sbert_similarity": float(sbert_sim),
                    "simple_similarity": float(simple_sim),
                    "sbert_threshold_met": sbert_sim >= self.similarity_threshold,
                    "simple_threshold_met": simple_sim >= self.low_similarity_threshold,
                    "would_match": sbert_sim >= self.similarity_threshold or simple_sim >= self.low_similarity_threshold
                }
                
                debug_info["candidates"].append(candidate_info)
        
        # Sortuj wedÅ‚ug najlepszego wyniku
        debug_info["candidates"].sort(key=lambda x: max(x["sbert_similarity"], x["simple_similarity"]), reverse=True)
        
        return debug_info

    def analyze_similarity_details(self, file1: str, file2: str) -> Dict:
        """
        SzczegÃ³Å‚owa analiza podobieÅ„stwa miÄ™dzy dwoma plikami
        """
        processed1 = self.preprocess_filename(file1)
        processed2 = self.preprocess_filename(file2)

        # Oblicz embeddings
        embeddings = self.model.encode([processed1, processed2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

        # Analiza sÅ‚Ã³w kluczowych
        words1 = set(processed1.lower().split())
        words2 = set(processed2.lower().split())

        common_words = words1.intersection(words2)
        word_overlap = len(common_words) / max(len(words1.union(words2)), 1)

        analysis = {
            "file1": file1,
            "file2": file2,
            "processed1": processed1,
            "processed2": processed2,
            "sbert_similarity": float(similarity),
            "common_words": list(common_words),
            "word_overlap_ratio": float(word_overlap),
            "words_file1": list(words1),
            "words_file2": list(words2),
        }

        return analysis

    def debug_specific_case(self, archive_name: str, image_name: str) -> Dict:
        """
        SzczegÃ³Å‚owe debugowanie konkretnego przypadku
        """
        debug_result = {
            "archive_name": archive_name,
            "image_name": image_name,
            "preprocessing": {
                "archive_processed": self.preprocess_filename(archive_name),
                "image_processed": self.preprocess_filename(image_name)
            }
        }
        
        # Test SBERT
        try:
            embeddings = self.calculate_embeddings([archive_name, image_name])
            if len(embeddings) >= 2:
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                debug_result["sbert_similarity"] = float(similarity)
                debug_result["sbert_threshold_met"] = similarity >= self.similarity_threshold
            else:
                debug_result["sbert_error"] = "Nie udaÅ‚o siÄ™ obliczyÄ‡ embeddings"
        except Exception as e:
            debug_result["sbert_error"] = str(e)
        
        # Test prostego dopasowania
        simple_sim = self.simple_string_similarity(archive_name, image_name)
        debug_result["simple_similarity"] = simple_sim
        debug_result["simple_threshold_met"] = simple_sim >= self.low_similarity_threshold
        
        # Test bardzo prostego dopasowania
        archive_clean = re.sub(r'[_\-\.\s]', '', os.path.splitext(archive_name)[0].lower())
        image_clean = re.sub(r'[_\-\.\s]', '', os.path.splitext(image_name)[0].lower())
        
        debug_result["very_simple"] = {
            "archive_clean": archive_clean,
            "image_clean": image_clean,
            "archive_in_image": archive_clean in image_clean,
            "image_in_archive": image_clean in archive_clean,
            "would_match": (archive_clean in image_clean or image_clean in archive_clean) and len(archive_clean) > 2
        }
        
        return debug_result


def get_work_directory_from_config():
    """Pobiera folder roboczy z konfiguracji lub None jeÅ›li nie ustawiony"""
    try:
        work_dir = config_manager.get_work_directory()
        if work_dir and os.path.isdir(work_dir):
            logger.info(f"ðŸ“ Znaleziono folder roboczy w konfiguracji: {work_dir}")
            return work_dir
        else:
            logger.warning("âš ï¸ Brak prawidÅ‚owego folderu roboczego w konfiguracji")
            return None
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania folderu roboczego z konfiguracji: {e}")
        return None


class AIFolderProcessor:
    """
    Klasa do przetwarzania folderÃ³w z wykorzystaniem AI
    """

    def __init__(self, enable_hardware_optimization: bool = True):
        """
        Inicjalizacja z opcjonalnÄ… optymalizacjÄ… sprzÄ™towÄ…
        """
        # UtwÃ³rz matcher z optymalizacjÄ… sprzÄ™towÄ…
        self.matcher = SBERTFileMatcher(auto_optimize=enable_hardware_optimization)
        
        # Pobierz folder roboczy z konfiguracji
        self.work_directory = get_work_directory_from_config()
        if not self.work_directory:
            logger.warning("Brak folderu roboczego w konfiguracji")
        
        # Loguj informacje o optymalizacji
        if enable_hardware_optimization and hasattr(self.matcher, 'hardware_config') and self.matcher.hardware_config:
            optimization_level = self.matcher.hardware_config.get('optimization_level', 'basic')
            device = self.matcher.hardware_config.get('device', 'cpu').upper()
            logger.info(f"ðŸš€ AI Procesor zoptymalizowany: {optimization_level} na {device}")

    def load_existing_index(self, folder_path: str) -> Dict:
        """
        Åaduje istniejÄ…cy plik index.json
        """
        index_path = os.path.join(folder_path, "index.json")

        if os.path.exists(index_path):
            try:
                with open(index_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"BÅ‚Ä…d odczytu {index_path}: {e}")
                return {}
        else:
            logger.info(f"Brak pliku index.json w {folder_path}")
            return {}

    def save_index_with_ai_data(self, folder_path: str, index_data: Dict):
        """
        Zapisuje plik index.json z danymi AI
        """
        index_path = os.path.join(folder_path, "index.json")

        try:
            with open(index_path, "w", encoding="utf-8") as f:
                json.dump(index_data, f, indent=4, ensure_ascii=False)
            logger.info(f"ðŸ’¾ Zapisano AI dane do: {index_path}")
        except Exception as e:
            logger.error(f"BÅ‚Ä…d zapisu {index_path}: {e}")

    def collect_files_in_folder(self, folder_path: str) -> Tuple[List[str], List[str]]:
        """
        Zbiera pliki archiwÃ³w i obrazÃ³w w folderze
        """
        archive_files = []
        image_files = []
        start_time = time.time()
        SCAN_TIMEOUT_SECONDS = 30  # Timeout dla skanowania pojedynczego folderu

        try:
            # Dodaj obsÅ‚ugÄ™ kodowania UTF-8
            folder_path = os.path.abspath(folder_path)
            
            for entry in os.scandir(folder_path):
                if time.time() - start_time > SCAN_TIMEOUT_SECONDS:
                    logger.warning(f"â° Przekroczono limit czasu skanowania ({SCAN_TIMEOUT_SECONDS}s) w folderze {folder_path}")
                    break

                try:
                    if entry.is_file(follow_symlinks=False):
                        # UÅ¼yj str() zamiast bezpoÅ›redniego odczytu name
                        filename = str(entry.name).lower()

                        if filename == "index.json":
                            continue

                        if filename.endswith(IMAGE_EXTENSIONS):
                            image_files.append(str(entry.name))
                        else:
                            # Wszystkie inne pliki traktujemy jako archiwa/modele
                            archive_files.append(str(entry.name))
                except UnicodeEncodeError as e:
                    logger.error(f"BÅ‚Ä…d kodowania nazwy pliku w {folder_path}: {e}")
                    continue
                except OSError as e:
                    logger.error(f"BÅ‚Ä…d dostÄ™pu do pliku w {folder_path}: {e}")
                    continue

        except OSError as e:
            logger.error(f"BÅ‚Ä…d skanowania folderu {folder_path}: {e}")
        except Exception as e:
            logger.error(f"Nieoczekiwany bÅ‚Ä…d podczas skanowania {folder_path}: {e}")

        return archive_files, image_files

    def process_folder(self, folder_path: str, progress_callback=None) -> bool:
        """
        Przetwarza jeden folder - dodaje dane AI do index.json
        """
        logger.info(f"ðŸ” Przetwarzanie AI folderu: {folder_path}")

        if progress_callback:
            progress_callback(f"ðŸ” Przetwarzanie AI folderu: {folder_path}")

        if not os.path.isdir(folder_path):
            logger.error(f"âŒ ÅšcieÅ¼ka nie jest folderem: {folder_path}")
            return False

        # SprawdÅº czy istnieje index.json
        index_json_path = os.path.join(folder_path, "index.json")
        if not os.path.exists(index_json_path):
            # UtwÃ³rz podstawowy index.json jeÅ›li nie istnieje
            logger.info(f"ðŸ“ TworzÄ™ nowy index.json dla: {folder_path}")
            if progress_callback:
                progress_callback(f"ðŸ“ TworzÄ™ nowy index.json dla: {folder_path}")
            
            basic_index_data = {
                "folder_info": {
                    "path": os.path.abspath(folder_path),
                    "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "total_size_bytes": 0,
                    "file_count": 0,
                    "subdir_count": 0,
                    "archive_count": 0
                },
                "files_with_previews": [],
                "files_without_previews": [],
                "other_images": []
            }
            
            try:
                with open(index_json_path, "w", encoding="utf-8") as f:
                    json.dump(basic_index_data, f, indent=4, ensure_ascii=False)
                logger.info(f"âœ… Utworzono index.json dla: {folder_path}")
            except Exception as e:
                logger.error(f"âŒ BÅ‚Ä…d tworzenia index.json dla {folder_path}: {e}")
                return False

        # Zbierz pliki
        archive_files, image_files = self.collect_files_in_folder(folder_path)

        if not archive_files and not image_files:
            logger.info(f"âš ï¸ Folder pusty (brak plikÃ³w do analizy AI): {folder_path}")
            if progress_callback:
                progress_callback(
                    f"âš ï¸ Folder pusty (brak plikÃ³w do analizy AI): {folder_path}"
                )
            return True

        logger.info(
            f"ðŸ“Š Znaleziono: {len(archive_files)} archiwÃ³w, {len(image_files)} obrazÃ³w"
        )

        # ZaÅ‚aduj istniejÄ…cy index.json
        index_data = self.load_existing_index(folder_path)

        # SprawdÅº czy AI juÅ¼ przetwarzaÅ‚o ten folder
        if "AI_processing_date" in index_data:
            logger.info(f"ðŸ”„ AktualizujÄ™ istniejÄ…ce dane AI dla: {folder_path}")
            if progress_callback:
                progress_callback(
                    f"ðŸ”„ AktualizujÄ™ istniejÄ…ce dane AI dla: {folder_path}"
                )
        else:
            logger.info(f"ðŸ†• Pierwsze przetwarzanie AI dla: {folder_path}")
            if progress_callback:
                progress_callback(f"ðŸ†• Pierwsze przetwarzanie AI dla: {folder_path}")

        # JeÅ›li nie ma podstawowej struktury, utwÃ³rz jÄ…
        if "folder_info" not in index_data:
            index_data["folder_info"] = {
                "path": os.path.abspath(folder_path),
                "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        # Dodaj sekcjÄ™ AI
        ai_data = {
            "AI_processing_date": datetime.now().isoformat(),
            "AI_model_info": {
                "name": "sentence-transformers/all-MiniLM-L6-v2",
                "type": "SBERT",
                "version": "1.0",
            },
            "AI_file_analysis": {
                "total_archive_files": len(archive_files),
                "total_image_files": len(image_files),
                "archive_files": archive_files,
                "image_files": image_files,
            },
        }

        # ZnajdÅº dopasowania AI
        if archive_files and image_files:
            logger.info("ðŸ¤– Uruchamiam analizÄ™ AI...")
            if progress_callback:
                progress_callback("ðŸ¤– Uruchamiam analizÄ™ AI...")

            start_time = time.time()

            matches = self.matcher.find_best_matches(archive_files, image_files)

            ai_time = time.time() - start_time
            logger.info(f"â±ï¸ Analiza AI zakoÅ„czona w {ai_time:.2f}s")
            if progress_callback:
                progress_callback(f"â±ï¸ Analiza AI zakoÅ„czona w {ai_time:.2f}s")

            ai_data["AI_matches"] = matches
            ai_data["AI_statistics"] = {
                "total_possible_pairs": len(archive_files) * len(image_files),
                "found_matches": len(matches),
                "match_rate": len(matches) / len(archive_files) if archive_files else 0,
                "processing_time_seconds": ai_time,
                "high_confidence_matches": len(
                    [m for m in matches if m["confidence_level"] == "HIGH"]
                ),
                "medium_confidence_matches": len(
                    [m for m in matches if m["confidence_level"] == "MEDIUM"]
                ),
            }

            # Dodaj szczegÃ³Å‚owe analizy dla najlepszych dopasowaÅ„
            detailed_analyses = []
            for match in matches[:3]:  # Tylko 3 najlepsze dla oszczÄ™dnoÅ›ci miejsca
                analysis = self.matcher.analyze_similarity_details(
                    match["archive_file"], match["image_file"]
                )
                detailed_analyses.append(analysis)

            ai_data["AI_detailed_analysis_samples"] = detailed_analyses

            if progress_callback:
                progress_callback(f"âœ… Znaleziono {len(matches)} dopasowaÅ„ AI")

        else:
            ai_data["AI_matches"] = []
            ai_data["AI_statistics"] = {
                "total_possible_pairs": 0,
                "found_matches": 0,
                "match_rate": 0,
                "reason": "Brak plikÃ³w archiwÃ³w lub obrazÃ³w do dopasowania",
            }

        # Dodaj dane AI do index_data
        for key, value in ai_data.items():
            index_data[key] = value

        # Zapisz plik
        self.save_index_with_ai_data(folder_path, index_data)

        return True

    def process_folder_recursive(self, root_folder_path: str, progress_callback=None):
        """
        Przetwarza folder rekurencyjnie (Å‚Ä…cznie z podfolderami)
        """
        logger.info(f"ðŸš€ Rozpoczynam rekurencyjne przetwarzanie AI: {root_folder_path}")

        if progress_callback:
            progress_callback(
                f"ðŸš€ Rozpoczynam rekurencyjne przetwarzanie AI: {root_folder_path}"
            )

        processed_folders = 0
        error_folders = 0

        try:
            # Dodaj obsÅ‚ugÄ™ kodowania UTF-8
            root_folder_path = os.path.abspath(root_folder_path)
            
            for root, dirs, files in os.walk(root_folder_path, onerror=lambda e: logger.error(f"BÅ‚Ä…d podczas chodzenia po katalogu: {e}")):
                try:
                    # PomiÅ„ linki symboliczne
                    if os.path.islink(root):
                        continue

                    # Konwertuj Å›cieÅ¼kÄ™ na UTF-8
                    root = str(root)
                    
                    # SprawdÅº czy folder zawiera index.json
                    index_json_path = os.path.join(root, "index.json")
                    if not os.path.exists(index_json_path):
                        # UtwÃ³rz podstawowy index.json jeÅ›li nie istnieje
                        logger.info(f"ðŸ“ TworzÄ™ nowy index.json dla: {root}")
                        if progress_callback:
                            progress_callback(f"ðŸ“ TworzÄ™ nowy index.json dla: {root}")
                        
                        basic_index_data = {
                            "folder_info": {
                                "path": os.path.abspath(root),
                                "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                "total_size_bytes": 0,
                                "file_count": 0,
                                "subdir_count": 0,
                                "archive_count": 0
                            },
                            "files_with_previews": [],
                            "files_without_previews": [],
                            "other_images": []
                        }
                        
                        try:
                            with open(index_json_path, "w", encoding="utf-8") as f:
                                json.dump(basic_index_data, f, indent=4, ensure_ascii=False)
                            logger.info(f"âœ… Utworzono index.json dla: {root}")
                        except Exception as e:
                            logger.error(f"âŒ BÅ‚Ä…d tworzenia index.json dla {root}: {e}")
                            error_folders += 1
                            continue

                    logger.info(f"ðŸ“ Przetwarzam AI dla folderu: {root}")
                    if progress_callback:
                        progress_callback(f"ðŸ“ Przetwarzam AI dla folderu: {root}")

                    if self.process_folder(root, progress_callback):
                        processed_folders += 1
                    else:
                        error_folders += 1

                except UnicodeEncodeError as e:
                    logger.error(f"BÅ‚Ä…d kodowania w folderze {root}: {e}")
                    error_folders += 1
                    continue
                except OSError as e:
                    logger.error(f"BÅ‚Ä…d systemowy w folderze {root}: {e}")
                    error_folders += 1
                    continue
                except Exception as e:
                    logger.error(f"Nieoczekiwany bÅ‚Ä…d w folderze {root}: {e}")
                    error_folders += 1
                    continue

        except Exception as e:
            logger.error(f"Krytyczny bÅ‚Ä…d podczas rekurencyjnego przetwarzania: {e}")
            if progress_callback:
                progress_callback(f"âŒ Krytyczny bÅ‚Ä…d: {e}")

        success_msg = f"âœ… Przetwarzanie AI zakoÅ„czone: {processed_folders} folderÃ³w OK, {error_folders} bÅ‚Ä™dÃ³w"
        logger.info(success_msg)
        if progress_callback:
            progress_callback(success_msg)

        return processed_folders > 0

    def start_ai_processing(self, progress_callback=None):
        """Rozpoczyna przetwarzanie AI od folderu roboczego z konfiguracji"""
        if not self.work_directory:
            logger.error("Brak folderu roboczego w konfiguracji")
            if progress_callback:
                progress_callback("âŒ Brak folderu roboczego w konfiguracji")
            return False

        if not os.path.isdir(self.work_directory):
            logger.error(f"Folder roboczy nie istnieje: {self.work_directory}")
            if progress_callback:
                progress_callback(
                    f"âŒ Folder roboczy nie istnieje: {self.work_directory}"
                )
            return False

        logger.info(f"ðŸ¤– Rozpoczynam przetwarzanie AI dla: {self.work_directory}")
        if progress_callback:
            progress_callback(
                f"ðŸ¤– Rozpoczynam przetwarzanie AI dla: {self.work_directory}"
            )

        return self.process_folder_recursive(self.work_directory, progress_callback)


def main():
    """
    GÅ‚Ã³wna funkcja programu
    """
    print("\nðŸ¤– AI File Matcher - Dopasowywanie plikÃ³w")
    print("=" * 50)
    
    while True:
        print("\nðŸ“‹ DostÄ™pne opcje:")
        print("1. PrzetwÃ³rz folder")
        print("2. PrzetwÃ³rz folder rekurencyjnie")
        print("3. Wygeneruj galeriÄ™ AI")
        print("4. WyjÅ›cie")
        print("5. Test konkretnego przypadku")
        
        choice = input("\nWybierz opcjÄ™ (1-5): ").strip()
        
        if choice == "1":
            folder_path = input("\nPodaj Å›cieÅ¼kÄ™ do folderu: ").strip()
            if folder_path:
                processor = AIFolderProcessor()
                processor.process_folder(folder_path)
            else:
                print("âŒ Nie podano Å›cieÅ¼ki")
                
        elif choice == "2":
            folder_path = input("\nPodaj Å›cieÅ¼kÄ™ do folderu gÅ‚Ã³wnego: ").strip()
            if folder_path:
                processor = AIFolderProcessor()
                processor.process_folder_recursive(folder_path)
            else:
                print("âŒ Nie podano Å›cieÅ¼ki")
                
        elif choice == "3":
            folder_path = input("\nPodaj Å›cieÅ¼kÄ™ do folderu: ").strip()
            if folder_path:
                gallery_data = generate_ai_only_gallery_data(folder_path)
                print("\nâœ… Galeria AI wygenerowana")
            else:
                print("âŒ Nie podano Å›cieÅ¼ki")
                
        elif choice == "4":
            print("\nðŸ‘‹ Do widzenia!")
            break
            
        elif choice == "5":
            # Test konkretnego przypadku
            print("\nðŸ” Test konkretnego przypadku:")
            archive_name = input("Podaj nazwÄ™ pliku archiwum: ").strip()
            image_name = input("Podaj nazwÄ™ pliku obrazu: ").strip()
            
            if archive_name and image_name:
                processor = AIFolderProcessor()
                debug_result = processor.matcher.debug_specific_case(archive_name, image_name)
                print("\nðŸ“Š WYNIKI DEBUGOWANIA:")
                print("-" * 50)
                import json
                print(json.dumps(debug_result, indent=2, ensure_ascii=False))
            else:
                print("âŒ Nie podano nazw plikÃ³w")
                
        else:
            print("âŒ NieprawidÅ‚owa opcja")


def generate_ai_only_gallery_data(folder_path: str) -> Dict:
    """
    Generuje dane galerii zawierajÄ…ce tylko dopasowania AI
    """
    index_path = os.path.join(folder_path, "index.json")

    if not os.path.exists(index_path):
        logger.warning(f"Brak index.json w {folder_path}")
        return {}

    try:
        with open(index_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"BÅ‚Ä…d odczytu index.json w {folder_path}: {e}")
        return {}

    # SprawdÅº czy sÄ… dane AI
    if "AI_matches" not in data:
        logger.debug(f"Brak danych AI_matches w {folder_path}")
        return {}

    ai_matches = data.get("AI_matches", [])
    if not ai_matches:
        logger.debug(f"Pusta lista AI_matches w {folder_path}")
        return {}

    logger.info(
        f"GenerujÄ™ dane galerii AI dla {folder_path} z {len(ai_matches)} dopasowaniami"
    )

    # UtwÃ³rz strukturÄ™ danych tylko z dopasowaniami AI
    ai_gallery_data = {
        "folder_info": data.get("folder_info", {}).copy(),
        "files_with_previews": [],
        "files_without_previews": [],
        "other_images": [],
        "ai_info": {
            "processing_date": data.get("AI_processing_date"),
            "model_info": data.get("AI_model_info", {}),
            "statistics": data.get("AI_statistics", {}),
            "total_matches": len(ai_matches),
        },
    }

    # Dodaj informacje AI do folder_info
    ai_gallery_data["folder_info"]["gallery_type"] = "AI_POWERED"
    ai_gallery_data["folder_info"]["ai_matches_count"] = len(ai_matches)
    ai_gallery_data["folder_info"]["ai_scan_date"] = data.get("AI_processing_date")

    # Konwertuj dopasowania AI na format galerii
    processed_matches = 0
    for match in ai_matches:
        archive_file = match.get("archive_file")
        image_file = match.get("image_file")

        if not archive_file or not image_file:
            logger.warning(f"NiepeÅ‚ne dopasowanie AI: {match}")
            continue

        # ZnajdÅº peÅ‚ne Å›cieÅ¼ki plikÃ³w
        archive_path = os.path.join(folder_path, archive_file)
        image_path = os.path.join(folder_path, image_file)

        if not os.path.exists(archive_path):
            logger.warning(f"Plik archiwum nie istnieje: {archive_path}")
            continue

        if not os.path.exists(image_path):
            logger.warning(f"Plik obrazu nie istnieje: {image_path}")
            continue

        try:
            archive_size = os.path.getsize(archive_path)
        except OSError:
            archive_size = 0

        file_info = {
            "name": archive_file,
            "path_absolute": os.path.abspath(archive_path),
            "size_bytes": archive_size,
            "size_readable": get_file_size_readable_ai(archive_size),
            "preview_found": True,
            "preview_name": image_file,
            "preview_path_absolute": os.path.abspath(image_path),
            "preview_relative_path": f"file:///{os.path.abspath(image_path).replace(os.sep, '/')}",
            "archive_link": f"file:///{os.path.abspath(archive_path).replace(os.sep, '/')}",
            "ai_match": True,
            "ai_confidence": match.get("confidence_level", "UNKNOWN"),
            "ai_similarity_score": match.get("similarity_score", 0.0),
            "ai_matching_method": match.get("matching_method", "SBERT"),
            "ai_timestamp": match.get("timestamp"),
        }

        # Dodaj kolor archiwum jeÅ›li dostÄ™pny
        file_ext = os.path.splitext(archive_file)[1].lower()
        try:
            import config_manager

            file_info["archive_color"] = config_manager.get_archive_color(file_ext)
        except:
            file_info["archive_color"] = "#6c757d"

        ai_gallery_data["files_with_previews"].append(file_info)
        processed_matches += 1

    logger.info(
        f"Przetworzone {processed_matches}/{len(ai_matches)} dopasowaÅ„ AI dla {folder_path}"
    )

    return ai_gallery_data


def get_file_size_readable_ai(size_bytes):
    """Funkcja pomocnicza do konwersji rozmiaru pliku dla AI"""
    if size_bytes == 0:
        return "0 B"
    size_name = ("B", "KB", "MB", "GB", "TB")
    i = 0
    size_float = float(size_bytes)
    while size_float >= 1024 and i < len(size_name) - 1:
        size_float /= 1024.0
        i += 1
    return f"{size_float:.2f} {size_name[i]}"


if __name__ == "__main__":
    main()
