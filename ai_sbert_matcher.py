# ai_sbert_matcher.py
import json
import logging
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

# Potrzebne biblioteki - zainstaluj przez: pip install sentence-transformers numpy scikit-learn
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import config_manager

# Konfiguracja loggera
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rozszerzenia obrazÃ³w
IMAGE_EXTENSIONS = (
    ".jpg",
    ".jpeg",
    ".jpe",
    ".jfif",
    ".png",
    ".apng",
    ".gif",
    ".bmp",
    ".dib",
    ".webp",
    ".tiff",
    ".tif",
    ".svg",
    ".svgz",
    ".ico",
    ".avif",
    ".heic",
    ".heif",
)


class SBERTFileMatcher:
    """
    Klasa do dopasowywania nazw plikÃ³w uÅ¼ywajÄ…c Sentence-BERT
    """

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Inicjalizacja z wyborem modelu SBERT

        Modele do wyboru:
        - 'all-MiniLM-L6-v2' - szybki, dobry stosunek jakoÅ›Ä‡/prÄ™dkoÅ›Ä‡ (22MB)
        - 'all-MiniLM-L12-v2' - wiÄ™kszy, lepszy (43MB)
        - 'paraphrase-MiniLM-L6-v2' - dobry do parafraz (22MB)
        """
        logger.info(f"Åadowanie modelu SBERT: {model_name}")
        start_time = time.time()

        try:
            self.model = SentenceTransformer(model_name)
            load_time = time.time() - start_time
            logger.info(f"Model zaÅ‚adowany w {load_time:.2f}s")
        except Exception as e:
            logger.error(f"BÅ‚Ä…d Å‚adowania modelu: {e}")
            raise

        # Parametry dopasowywania
        self.similarity_threshold = 0.65  # Minimalny prÃ³g podobieÅ„stwa
        self.high_confidence_threshold = 0.80  # PrÃ³g wysokiej pewnoÅ›ci

    def preprocess_filename(self, filename: str) -> str:
        """
        Przygotowuje nazwÄ™ pliku do analizy przez model
        """
        # UsuÅ„ rozszerzenie
        name_without_ext = os.path.splitext(filename)[0]

        # ZamieÅ„ rÃ³Å¼ne separatory na spacje
        processed = re.sub(r"[_\-\.]", " ", name_without_ext)

        # UsuÅ„ wielokrotne spacje
        processed = re.sub(r"\s+", " ", processed)

        # Wydziel i oznacz typowe sufiksy
        suffixes = [
            "preview",
            "thumb",
            "thumbnail",
            "render",
            "final",
            "draft",
            "v1",
            "v2",
            "v3",
            "version",
            "copy",
            "backup",
            "temp",
        ]

        for suffix in suffixes:
            if suffix in processed.lower():
                processed = processed.replace(suffix.lower(), f" {suffix.upper()}")
                processed = processed.replace(suffix.upper(), f" {suffix.upper()}")

        # Wydziel numery wersji
        processed = re.sub(r"(\d+)", r" \1 ", processed)

        # CzyÅ›Ä‡ wielokrotne spacje
        processed = re.sub(r"\s+", " ", processed).strip()

        logger.debug(f"Preprocessing: '{filename}' -> '{processed}'")
        return processed

    def calculate_embeddings(self, filenames: List[str]) -> np.ndarray:
        """
        Oblicza embeddings dla listy nazw plikÃ³w
        """
        if not filenames:
            return np.array([])

        # Przetwarzaj nazwy plikÃ³w
        processed_names = [self.preprocess_filename(name) for name in filenames]

        logger.debug(f"Obliczanie embeddings dla {len(processed_names)} plikÃ³w")
        start_time = time.time()

        # Oblicz embeddings
        embeddings = self.model.encode(processed_names, show_progress_bar=False)

        calc_time = time.time() - start_time
        logger.debug(f"Embeddings obliczone w {calc_time:.2f}s")

        return embeddings

    def find_best_matches(
        self, archive_files: List[str], image_files: List[str]
    ) -> List[Dict]:
        """
        Znajduje najlepsze dopasowania miÄ™dzy plikami archiwÃ³w a obrazami
        """
        if not archive_files or not image_files:
            logger.warning("Brak plikÃ³w do dopasowania")
            return []

        logger.info(
            f"Szukanie dopasowaÅ„: {len(archive_files)} archiwÃ³w vs {len(image_files)} obrazÃ³w"
        )

        # Oblicz embeddings
        archive_embeddings = self.calculate_embeddings(archive_files)
        image_embeddings = self.calculate_embeddings(image_files)

        # Oblicz podobieÅ„stwa
        similarities = cosine_similarity(archive_embeddings, image_embeddings)

        matches = []
        used_images = set()

        # Dla kaÅ¼dego archiwum znajdÅº najlepszy obraz
        for i, archive_file in enumerate(archive_files):
            best_similarity = 0.0
            best_image_idx = -1

            for j, image_file in enumerate(image_files):
                if j in used_images:
                    continue  # Ten obraz juÅ¼ zostaÅ‚ uÅ¼yty

                similarity = similarities[i][j]
                if (
                    similarity > best_similarity
                    and similarity >= self.similarity_threshold
                ):
                    best_similarity = similarity
                    best_image_idx = j

            if best_image_idx != -1:
                image_file = image_files[best_image_idx]
                used_images.add(best_image_idx)

                confidence_level = (
                    "HIGH"
                    if best_similarity >= self.high_confidence_threshold
                    else "MEDIUM"
                )

                match_info = {
                    "archive_file": archive_file,
                    "image_file": image_file,
                    "similarity_score": float(best_similarity),
                    "confidence_level": confidence_level,
                    "matching_method": "SBERT",
                    "timestamp": datetime.now().isoformat(),
                }

                matches.append(match_info)
                logger.info(
                    f"âœ… Dopasowanie [{confidence_level}]: '{archive_file}' â†” '{image_file}' (score: {best_similarity:.3f})"
                )
            else:
                logger.debug(f"âŒ Brak dopasowania dla: '{archive_file}'")

        logger.info(
            f"Znaleziono {len(matches)} dopasowaÅ„ z {len(archive_files)} archiwÃ³w"
        )
        return matches

    def analyze_similarity_details(self, file1: str, file2: str) -> Dict:
        """
        SzczegÃ³Å‚owa analiza podobieÅ„stwa miÄ™dzy dwoma plikami
        """
        processed1 = self.preprocess_filename(file1)
        processed2 = self.preprocess_filename(file2)

        # Oblicz embeddings
        embeddings = self.model.encode([processed1, processed2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

        # Analiza sÅ‚Ã³w kluczowych
        words1 = set(processed1.lower().split())
        words2 = set(processed2.lower().split())

        common_words = words1.intersection(words2)
        word_overlap = len(common_words) / max(len(words1.union(words2)), 1)

        analysis = {
            "file1": file1,
            "file2": file2,
            "processed1": processed1,
            "processed2": processed2,
            "sbert_similarity": float(similarity),
            "common_words": list(common_words),
            "word_overlap_ratio": float(word_overlap),
            "words_file1": list(words1),
            "words_file2": list(words2),
        }

        return analysis


def get_work_directory_from_config():
    """Pobiera folder roboczy z konfiguracji lub None jeÅ›li nie ustawiony"""
    try:
        work_dir = config_manager.get_work_directory()
        if work_dir and os.path.isdir(work_dir):
            logger.info(f"ğŸ“ Znaleziono folder roboczy w konfiguracji: {work_dir}")
            return work_dir
        else:
            logger.warning("âš ï¸ Brak prawidÅ‚owego folderu roboczego w konfiguracji")
            return None
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania folderu roboczego z konfiguracji: {e}")
        return None


class AIFolderProcessor:
    """
    Klasa do przetwarzania folderÃ³w i dodawania danych AI do index.json
    """

    def __init__(self):
        self.matcher = SBERTFileMatcher()
        # Pobierz folder roboczy z konfiguracji
        self.work_directory = get_work_directory_from_config()
        if not self.work_directory:
            logger.warning("Brak folderu roboczego w konfiguracji")

    def load_existing_index(self, folder_path: str) -> Dict:
        """
        Åaduje istniejÄ…cy plik index.json
        """
        index_path = os.path.join(folder_path, "index.json")

        if os.path.exists(index_path):
            try:
                with open(index_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"BÅ‚Ä…d odczytu {index_path}: {e}")
                return {}
        else:
            logger.info(f"Brak pliku index.json w {folder_path}")
            return {}

    def save_index_with_ai_data(self, folder_path: str, index_data: Dict):
        """
        Zapisuje plik index.json z danymi AI
        """
        index_path = os.path.join(folder_path, "index.json")

        try:
            with open(index_path, "w", encoding="utf-8") as f:
                json.dump(index_data, f, indent=4, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Zapisano AI dane do: {index_path}")
        except Exception as e:
            logger.error(f"BÅ‚Ä…d zapisu {index_path}: {e}")

    def collect_files_in_folder(self, folder_path: str) -> Tuple[List[str], List[str]]:
        """
        Zbiera pliki archiwÃ³w i obrazÃ³w w folderze
        """
        archive_files = []
        image_files = []

        try:
            for entry in os.scandir(folder_path):
                if entry.is_file(follow_symlinks=False):
                    filename = entry.name.lower()

                    if filename == "index.json":
                        continue

                    if filename.endswith(IMAGE_EXTENSIONS):
                        image_files.append(entry.name)
                    else:
                        # Wszystkie inne pliki traktujemy jako archiwa/modele
                        archive_files.append(entry.name)

        except OSError as e:
            logger.error(f"BÅ‚Ä…d skanowania folderu {folder_path}: {e}")

        return archive_files, image_files

    def process_folder(self, folder_path: str, progress_callback=None) -> bool:
        """
        Przetwarza jeden folder - dodaje dane AI do index.json
        """
        logger.info(f"ğŸ” Przetwarzanie AI folderu: {folder_path}")

        if progress_callback:
            progress_callback(f"ğŸ” Przetwarzanie AI folderu: {folder_path}")

        if not os.path.isdir(folder_path):
            logger.error(f"âŒ ÅšcieÅ¼ka nie jest folderem: {folder_path}")
            return False

        # SprawdÅº czy istnieje index.json (folder musi byÄ‡ juÅ¼ przeskanowany)
        index_json_path = os.path.join(folder_path, "index.json")
        if not os.path.exists(index_json_path):
            logger.warning(f"âš ï¸ Brak index.json w folderze: {folder_path}")
            if progress_callback:
                progress_callback(f"âš ï¸ Brak index.json w folderze: {folder_path}")
            return False

        # Zbierz pliki
        archive_files, image_files = self.collect_files_in_folder(folder_path)

        if not archive_files and not image_files:
            logger.info(f"âš ï¸ Folder pusty (brak plikÃ³w do analizy AI): {folder_path}")
            if progress_callback:
                progress_callback(
                    f"âš ï¸ Folder pusty (brak plikÃ³w do analizy AI): {folder_path}"
                )
            return True

        logger.info(
            f"ğŸ“Š Znaleziono: {len(archive_files)} archiwÃ³w, {len(image_files)} obrazÃ³w"
        )

        # ZaÅ‚aduj istniejÄ…cy index.json
        index_data = self.load_existing_index(folder_path)

        # SprawdÅº czy AI juÅ¼ przetwarzaÅ‚o ten folder
        if "AI_processing_date" in index_data:
            logger.info(f"ğŸ”„ AktualizujÄ™ istniejÄ…ce dane AI dla: {folder_path}")
            if progress_callback:
                progress_callback(
                    f"ğŸ”„ AktualizujÄ™ istniejÄ…ce dane AI dla: {folder_path}"
                )
        else:
            logger.info(f"ğŸ†• Pierwsze przetwarzanie AI dla: {folder_path}")
            if progress_callback:
                progress_callback(f"ğŸ†• Pierwsze przetwarzanie AI dla: {folder_path}")

        # JeÅ›li nie ma podstawowej struktury, utwÃ³rz jÄ…
        if "folder_info" not in index_data:
            index_data["folder_info"] = {
                "path": os.path.abspath(folder_path),
                "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        # Dodaj sekcjÄ™ AI
        ai_data = {
            "AI_processing_date": datetime.now().isoformat(),
            "AI_model_info": {
                "name": "sentence-transformers/all-MiniLM-L6-v2",
                "type": "SBERT",
                "version": "1.0",
            },
            "AI_file_analysis": {
                "total_archive_files": len(archive_files),
                "total_image_files": len(image_files),
                "archive_files": archive_files,
                "image_files": image_files,
            },
        }

        # ZnajdÅº dopasowania AI
        if archive_files and image_files:
            logger.info("ğŸ¤– Uruchamiam analizÄ™ AI...")
            if progress_callback:
                progress_callback("ğŸ¤– Uruchamiam analizÄ™ AI...")

            start_time = time.time()

            matches = self.matcher.find_best_matches(archive_files, image_files)

            ai_time = time.time() - start_time
            logger.info(f"â±ï¸ Analiza AI zakoÅ„czona w {ai_time:.2f}s")
            if progress_callback:
                progress_callback(f"â±ï¸ Analiza AI zakoÅ„czona w {ai_time:.2f}s")

            ai_data["AI_matches"] = matches
            ai_data["AI_statistics"] = {
                "total_possible_pairs": len(archive_files) * len(image_files),
                "found_matches": len(matches),
                "match_rate": len(matches) / len(archive_files) if archive_files else 0,
                "processing_time_seconds": ai_time,
                "high_confidence_matches": len(
                    [m for m in matches if m["confidence_level"] == "HIGH"]
                ),
                "medium_confidence_matches": len(
                    [m for m in matches if m["confidence_level"] == "MEDIUM"]
                ),
            }

            # Dodaj szczegÃ³Å‚owe analizy dla najlepszych dopasowaÅ„
            detailed_analyses = []
            for match in matches[:3]:  # Tylko 3 najlepsze dla oszczÄ™dnoÅ›ci miejsca
                analysis = self.matcher.analyze_similarity_details(
                    match["archive_file"], match["image_file"]
                )
                detailed_analyses.append(analysis)

            ai_data["AI_detailed_analysis_samples"] = detailed_analyses

            if progress_callback:
                progress_callback(f"âœ… Znaleziono {len(matches)} dopasowaÅ„ AI")

        else:
            ai_data["AI_matches"] = []
            ai_data["AI_statistics"] = {
                "total_possible_pairs": 0,
                "found_matches": 0,
                "match_rate": 0,
                "reason": "Brak plikÃ³w archiwÃ³w lub obrazÃ³w do dopasowania",
            }

        # Dodaj dane AI do index_data
        for key, value in ai_data.items():
            index_data[key] = value

        # Zapisz plik
        self.save_index_with_ai_data(folder_path, index_data)

        return True

    def process_folder_recursive(self, root_folder_path: str, progress_callback=None):
        """
        Przetwarza folder rekurencyjnie (Å‚Ä…cznie z podfolderami)
        """
        logger.info(f"ğŸš€ Rozpoczynam rekurencyjne przetwarzanie AI: {root_folder_path}")

        if progress_callback:
            progress_callback(
                f"ğŸš€ Rozpoczynam rekurencyjne przetwarzanie AI: {root_folder_path}"
            )

        processed_folders = 0
        error_folders = 0

        for root, dirs, files in os.walk(root_folder_path):
            # PomiÅ„ linki symboliczne
            if os.path.islink(root):
                continue

            # SprawdÅº czy folder zawiera index.json (zostaÅ‚ juÅ¼ przeskanowany)
            index_json_path = os.path.join(root, "index.json")
            if not os.path.exists(index_json_path):
                logger.debug(f"â­ï¸ Pomijam folder bez index.json: {root}")
                continue

            logger.info(f"ğŸ“ Przetwarzam AI dla folderu: {root}")
            if progress_callback:
                progress_callback(f"ğŸ“ Przetwarzam AI dla folderu: {root}")

            if self.process_folder(root, progress_callback):
                processed_folders += 1
            else:
                error_folders += 1

        success_msg = f"âœ… Przetwarzanie AI zakoÅ„czone: {processed_folders} folderÃ³w OK, {error_folders} bÅ‚Ä™dÃ³w"
        logger.info(success_msg)
        if progress_callback:
            progress_callback(success_msg)

        return processed_folders > 0

    def start_ai_processing(self, progress_callback=None):
        """Rozpoczyna przetwarzanie AI od folderu roboczego z konfiguracji"""
        if not self.work_directory:
            logger.error("Brak folderu roboczego w konfiguracji")
            if progress_callback:
                progress_callback("âŒ Brak folderu roboczego w konfiguracji")
            return False

        if not os.path.isdir(self.work_directory):
            logger.error(f"Folder roboczy nie istnieje: {self.work_directory}")
            if progress_callback:
                progress_callback(
                    f"âŒ Folder roboczy nie istnieje: {self.work_directory}"
                )
            return False

        logger.info(f"ğŸ¤– Rozpoczynam przetwarzanie AI dla: {self.work_directory}")
        if progress_callback:
            progress_callback(
                f"ğŸ¤– Rozpoczynam przetwarzanie AI dla: {self.work_directory}"
            )

        return self.process_folder_recursive(self.work_directory, progress_callback)


def main():
    """
    Funkcja gÅ‚Ã³wna - automatycznie pobiera folder roboczy z konfiguracji
    """
    print("ğŸ¤– AI SBERT File Matcher - Automatyczne przetwarzanie")
    print("=" * 60)

    # UtwÃ³rz procesor i sprawdÅº konfiguracjÄ™
    processor = AIFolderProcessor()

    if not processor.work_directory:
        print("âŒ Brak folderu roboczego w konfiguracji!")
        print("ğŸ’¡ Uruchom najpierw gÅ‚Ã³wnÄ… aplikacjÄ™ i ustaw folder roboczy.")
        return

    print(f"ğŸ“ Folder roboczy z konfiguracji: {processor.work_directory}")

    if not os.path.exists(processor.work_directory):
        print(f"âŒ Folder roboczy nie istnieje: {processor.work_directory}")
        return

    # Zapytaj o tryb przetwarzania
    print("\nğŸ”„ Tryby przetwarzania:")
    print("1. Automatyczne (caÅ‚y folder roboczy)")
    print("2. Konkretny folder")
    print("3. WyjÅ›cie")

    choice = input("\nWybierz opcjÄ™ (1-3): ").strip()

    if choice == "1":
        # Automatyczne przetwarzanie caÅ‚ego folderu roboczego
        print(f"\nğŸš€ Rozpoczynam automatyczne przetwarzanie AI...")
        processor.start_ai_processing(print)

    elif choice == "2":
        # Konkretny folder
        test_folder = input("Podaj Å›cieÅ¼kÄ™ do konkretnego folderu: ").strip()
        if not test_folder:
            print("âŒ Nie podano Å›cieÅ¼ki")
            return

        if not os.path.exists(test_folder):
            print(f"âŒ Folder nie istnieje: {test_folder}")
            return

        print(f"ğŸ” Przetwarzam konkretny folder: {test_folder}")
        processor.process_folder_recursive(test_folder, print)

    elif choice == "3":
        print("ğŸ‘‹ Do widzenia!")
        return
    else:
        print("âŒ NieprawidÅ‚owy wybÃ³r")
        return

    print("\nğŸ‰ Przetwarzanie AI zakoÅ„czone! SprawdÅº pliki index.json w folderach.")
    print("ğŸ” Wyszukaj klucze zaczynajÄ…ce siÄ™ od 'AI_' aby zobaczyÄ‡ wyniki.")


def generate_ai_only_gallery_data(folder_path: str) -> Dict:
    """
    Generuje dane galerii zawierajÄ…ce tylko dopasowania AI
    """
    index_path = os.path.join(folder_path, "index.json")

    if not os.path.exists(index_path):
        logger.warning(f"Brak index.json w {folder_path}")
        return {}

    try:
        with open(index_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"BÅ‚Ä…d odczytu index.json w {folder_path}: {e}")
        return {}

    # SprawdÅº czy sÄ… dane AI
    if "AI_matches" not in data:
        logger.debug(f"Brak danych AI_matches w {folder_path}")
        return {}

    ai_matches = data.get("AI_matches", [])
    if not ai_matches:
        logger.debug(f"Pusta lista AI_matches w {folder_path}")
        return {}

    logger.info(
        f"GenerujÄ™ dane galerii AI dla {folder_path} z {len(ai_matches)} dopasowaniami"
    )

    # UtwÃ³rz strukturÄ™ danych tylko z dopasowaniami AI
    ai_gallery_data = {
        "folder_info": data.get("folder_info", {}).copy(),
        "files_with_previews": [],
        "files_without_previews": [],
        "other_images": [],
        "ai_info": {
            "processing_date": data.get("AI_processing_date"),
            "model_info": data.get("AI_model_info", {}),
            "statistics": data.get("AI_statistics", {}),
            "total_matches": len(ai_matches),
        },
    }

    # Dodaj informacje AI do folder_info
    ai_gallery_data["folder_info"]["gallery_type"] = "AI_POWERED"
    ai_gallery_data["folder_info"]["ai_matches_count"] = len(ai_matches)
    ai_gallery_data["folder_info"]["ai_scan_date"] = data.get("AI_processing_date")

    # Konwertuj dopasowania AI na format galerii
    processed_matches = 0
    for match in ai_matches:
        archive_file = match.get("archive_file")
        image_file = match.get("image_file")

        if not archive_file or not image_file:
            logger.warning(f"NiepeÅ‚ne dopasowanie AI: {match}")
            continue

        # ZnajdÅº peÅ‚ne Å›cieÅ¼ki plikÃ³w
        archive_path = os.path.join(folder_path, archive_file)
        image_path = os.path.join(folder_path, image_file)

        if not os.path.exists(archive_path):
            logger.warning(f"Plik archiwum nie istnieje: {archive_path}")
            continue

        if not os.path.exists(image_path):
            logger.warning(f"Plik obrazu nie istnieje: {image_path}")
            continue

        try:
            archive_size = os.path.getsize(archive_path)
        except OSError:
            archive_size = 0

        file_info = {
            "name": archive_file,
            "path_absolute": os.path.abspath(archive_path),
            "size_bytes": archive_size,
            "size_readable": get_file_size_readable_ai(archive_size),
            "preview_found": True,
            "preview_name": image_file,
            "preview_path_absolute": os.path.abspath(image_path),
            "preview_relative_path": f"file:///{os.path.abspath(image_path).replace(os.sep, '/')}",
            "archive_link": f"file:///{os.path.abspath(archive_path).replace(os.sep, '/')}",
            "ai_match": True,
            "ai_confidence": match.get("confidence_level", "UNKNOWN"),
            "ai_similarity_score": match.get("similarity_score", 0.0),
            "ai_matching_method": match.get("matching_method", "SBERT"),
            "ai_timestamp": match.get("timestamp"),
        }

        # Dodaj kolor archiwum jeÅ›li dostÄ™pny
        file_ext = os.path.splitext(archive_file)[1].lower()
        try:
            import config_manager

            file_info["archive_color"] = config_manager.get_archive_color(file_ext)
        except:
            file_info["archive_color"] = "#6c757d"

        ai_gallery_data["files_with_previews"].append(file_info)
        processed_matches += 1

    logger.info(
        f"Przetworzone {processed_matches}/{len(ai_matches)} dopasowaÅ„ AI dla {folder_path}"
    )

    return ai_gallery_data


def get_file_size_readable_ai(size_bytes):
    """Funkcja pomocnicza do konwersji rozmiaru pliku dla AI"""
    if size_bytes == 0:
        return "0 B"
    size_name = ("B", "KB", "MB", "GB", "TB")
    i = 0
    size_float = float(size_bytes)
    while size_float >= 1024 and i < len(size_name) - 1:
        size_float /= 1024.0
        i += 1
    return f"{size_float:.2f} {size_name[i]}"


if __name__ == "__main__":
    main()
