# ai_sbert_matcher.py
import json
import logging
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

# Potrzebne biblioteki - zainstaluj przez: pip install sentence-transformers numpy scikit-learn
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import config_manager

# Konfiguracja loggera
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rozszerzenia obrazÃ³w
IMAGE_EXTENSIONS = (
    ".jpg",
    ".jpeg",
    ".jpe",
    ".jfif",
    ".png",
    ".apng",
    ".gif",
    ".bmp",
    ".dib",
    ".webp",
    ".tiff",
    ".tif",
    ".svg",
    ".svgz",
    ".ico",
    ".avif",
    ".heic",
    ".heif",
)


class SBERTFileMatcher:
    """
    Klasa do dopasowywania nazw plikÃ³w uÅ¼ywajÄ…c Sentence-BERT
    """

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Inicjalizacja z wyborem modelu SBERT

        Modele do wyboru:
        - 'all-MiniLM-L6-v2' - szybki, dobry stosunek jakoÅ›Ä‡/prÄ™dkoÅ›Ä‡ (22MB)
        - 'all-MiniLM-L12-v2' - wiÄ™kszy, lepszy (43MB)
        - 'paraphrase-MiniLM-L6-v2' - dobry do parafraz (22MB)
        """
        logger.info(f"Åadowanie modelu SBERT: {model_name}")
        start_time = time.time()

        try:
            self.model = SentenceTransformer(model_name)
            load_time = time.time() - start_time
            logger.info(f"Model zaÅ‚adowany w {load_time:.2f}s")
        except Exception as e:
            logger.error(f"BÅ‚Ä…d Å‚adowania modelu: {e}")
            raise

        # ObniÅ¼one progi dla lepszej czuÅ‚oÅ›ci
        self.similarity_threshold = 0.45  # Bardziej tolerancyjny prÃ³g
        self.high_confidence_threshold = 0.70  # Realistyczny prÃ³g wysokiej pewnoÅ›ci
        self.very_high_confidence_threshold = 0.85  # Dla wyjÄ…tkowych dopasowaÅ„

    def preprocess_filename(self, filename: str) -> str:
        """
        Ulepszone przetwarzanie nazw plikÃ³w z zachowaniem kluczowych informacji
        """
        # UsuÅ„ rozszerzenie
        name_without_ext = os.path.splitext(filename)[0]
        
        # Zachowaj oryginalne ID i numery seryjne
        # ZamieÅ„ separatory na spacje, ale zachowaj numery
        processed = re.sub(r"[_\-]", " ", name_without_ext)
        processed = re.sub(r"\.(?=\D)", " ", processed)  # Kropki tylko przed literami
        
        # Zachowaj dÅ‚ugie numery (prawdopodobnie ID)
        processed = re.sub(r"(\d{6,})", r" ID\1 ", processed)
        
        # Oznacz krÃ³tkie numery jako wersje
        processed = re.sub(r"\b(\d{1,3})\b", r" VER\1 ", processed)
        
        # WyczyÅ›Ä‡ wielokrotne spacje
        processed = re.sub(r"\s+", " ", processed).strip()
        
        logger.debug(f"Preprocessing: '{filename}' -> '{processed}'")
        return processed

    def simple_string_similarity(self, str1: str, str2: str) -> float:
        """
        Prosta miara podobieÅ„stwa bazujÄ…ca na wspÃ³lnych sÅ‚owach
        UÅ¼yteczna jako fallback gdy SBERT zawodzi
        """
        words1 = set(self.preprocess_filename(str1).lower().split())
        words2 = set(self.preprocess_filename(str2).lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccard similarity
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        jaccard = intersection / union if union > 0 else 0.0
        
        # Bonus za dÅ‚ugie wspÃ³lne czÄ™Å›ci
        str1_clean = self.preprocess_filename(str1).replace(" ", "").lower()
        str2_clean = self.preprocess_filename(str2).replace(" ", "").lower()
        
        # Longest common substring
        lcs_length = self.longest_common_substring_length(str1_clean, str2_clean)
        lcs_bonus = lcs_length / max(len(str1_clean), len(str2_clean))
        
        return min(1.0, jaccard + (lcs_bonus * 0.3))

    def longest_common_substring_length(self, str1: str, str2: str) -> int:
        """Znajduje dÅ‚ugoÅ›Ä‡ najdÅ‚uÅ¼szego wspÃ³lnego podciÄ…gu"""
        m, n = len(str1), len(str2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        max_length = 0
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if str1[i-1] == str2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                    max_length = max(max_length, dp[i][j])
                else:
                    dp[i][j] = 0
        
        return max_length

    def calculate_embeddings(self, filenames: List[str]) -> np.ndarray:
        """
        Oblicza embeddings dla listy nazw plikÃ³w
        """
        if not filenames:
            return np.array([])

        # Przetwarzaj nazwy plikÃ³w
        processed_names = [self.preprocess_filename(name) for name in filenames]

        logger.debug(f"Obliczanie embeddings dla {len(processed_names)} plikÃ³w")
        start_time = time.time()

        # Oblicz embeddings
        embeddings = self.model.encode(processed_names, show_progress_bar=False)

        calc_time = time.time() - start_time
        logger.debug(f"Embeddings obliczone w {calc_time:.2f}s")

        return embeddings

    def find_best_matches(self, archive_files: List[str], image_files: List[str]) -> List[Dict]:
        """
        Ulepszone dopasowywanie z warstwÄ… fallback
        """
        if not archive_files or not image_files:
            logger.warning("Brak plikÃ³w do dopasowania")
            return []

        logger.info(f"Szukanie dopasowaÅ„: {len(archive_files)} archiwÃ³w vs {len(image_files)} obrazÃ³w")

        # Oblicz embeddings SBERT
        archive_embeddings = self.calculate_embeddings(archive_files)
        image_embeddings = self.calculate_embeddings(image_files)
        similarities = cosine_similarity(archive_embeddings, image_embeddings)

        matches = []
        used_images = set()

        # Pierwsza faza: dopasowania SBERT
        for i, archive_file in enumerate(archive_files):
            best_similarity = 0.0
            best_image_idx = -1
            best_method = "SBERT"

            for j, image_file in enumerate(image_files):
                if j in used_images:
                    continue

                sbert_similarity = similarities[i][j]
                if sbert_similarity > best_similarity and sbert_similarity >= self.similarity_threshold:
                    best_similarity = sbert_similarity
                    best_image_idx = j

            # Druga faza: jeÅ›li SBERT nie znalazÅ‚ dopasowania, uÅ¼yj prostego algorytmu
            if best_image_idx == -1:
                logger.debug(f"SBERT nie znalazÅ‚ dopasowania dla '{archive_file}', prÃ³bujÄ™ prostego algorytmu")
                
                for j, image_file in enumerate(image_files):
                    if j in used_images:
                        continue

                    simple_similarity = self.simple_string_similarity(archive_file, image_file)
                    # NiÅ¼szy prÃ³g dla prostego algorytmu
                    if simple_similarity > best_similarity and simple_similarity >= 0.3:
                        best_similarity = simple_similarity
                        best_image_idx = j
                        best_method = "SIMPLE_STRING"

            if best_image_idx != -1:
                image_file = image_files[best_image_idx]
                used_images.add(best_image_idx)

                # OkreÅ›l poziom pewnoÅ›ci
                if best_similarity >= self.very_high_confidence_threshold:
                    confidence_level = "VERY_HIGH"
                elif best_similarity >= self.high_confidence_threshold:
                    confidence_level = "HIGH"
                elif best_similarity >= 0.50:
                    confidence_level = "MEDIUM"
                else:
                    confidence_level = "LOW"

                match_info = {
                    "archive_file": archive_file,
                    "image_file": image_file,
                    "similarity_score": float(best_similarity),
                    "confidence_level": confidence_level,
                    "matching_method": best_method,
                    "timestamp": datetime.now().isoformat(),
                }

                matches.append(match_info)
                logger.info(f"âœ… Dopasowanie [{confidence_level}][{best_method}]: '{archive_file}' â†” '{image_file}' (score: {best_similarity:.3f})")

        logger.info(f"Znaleziono {len(matches)} dopasowaÅ„ z {len(archive_files)} archiwÃ³w")
        return matches

    def debug_matching_process(self, archive_file: str, image_files: List[str]) -> Dict:
        """
        Funkcja debugowania pokazujÄ…ca dlaczego dopasowania mogÄ… nie dziaÅ‚aÄ‡
        """
        debug_info = {
            "archive_file": archive_file,
            "processed_archive": self.preprocess_filename(archive_file),
            "candidates": []
        }
        
        archive_embedding = self.calculate_embeddings([archive_file])
        image_embeddings = self.calculate_embeddings(image_files)
        
        if len(archive_embedding) > 0 and len(image_embeddings) > 0:
            similarities = cosine_similarity(archive_embedding, image_embeddings)[0]
            
            for i, image_file in enumerate(image_files):
                sbert_sim = similarities[i]
                simple_sim = self.simple_string_similarity(archive_file, image_file)
                
                candidate_info = {
                    "image_file": image_file,
                    "processed_image": self.preprocess_filename(image_file),
                    "sbert_similarity": float(sbert_sim),
                    "simple_similarity": float(simple_sim),
                    "sbert_threshold_met": sbert_sim >= self.similarity_threshold,
                    "simple_threshold_met": simple_sim >= 0.3,
                    "would_match": sbert_sim >= self.similarity_threshold or simple_sim >= 0.3
                }
                
                debug_info["candidates"].append(candidate_info)
        
        # Sortuj wedÅ‚ug najlepszego wyniku
        debug_info["candidates"].sort(key=lambda x: max(x["sbert_similarity"], x["simple_similarity"]), reverse=True)
        
        return debug_info

    def analyze_similarity_details(self, file1: str, file2: str) -> Dict:
        """
        SzczegÃ³Å‚owa analiza podobieÅ„stwa miÄ™dzy dwoma plikami
        """
        processed1 = self.preprocess_filename(file1)
        processed2 = self.preprocess_filename(file2)

        # Oblicz embeddings
        embeddings = self.model.encode([processed1, processed2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

        # Analiza sÅ‚Ã³w kluczowych
        words1 = set(processed1.lower().split())
        words2 = set(processed2.lower().split())

        common_words = words1.intersection(words2)
        word_overlap = len(common_words) / max(len(words1.union(words2)), 1)

        analysis = {
            "file1": file1,
            "file2": file2,
            "processed1": processed1,
            "processed2": processed2,
            "sbert_similarity": float(similarity),
            "common_words": list(common_words),
            "word_overlap_ratio": float(word_overlap),
            "words_file1": list(words1),
            "words_file2": list(words2),
        }

        return analysis


def get_work_directory_from_config():
    """Pobiera folder roboczy z konfiguracji lub None jeÅ›li nie ustawiony"""
    try:
        work_dir = config_manager.get_work_directory()
        if work_dir and os.path.isdir(work_dir):
            logger.info(f"ğŸ“ Znaleziono folder roboczy w konfiguracji: {work_dir}")
            return work_dir
        else:
            logger.warning("âš ï¸ Brak prawidÅ‚owego folderu roboczego w konfiguracji")
            return None
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania folderu roboczego z konfiguracji: {e}")
        return None


class AIFolderProcessor:
    """
    Klasa do przetwarzania folderÃ³w i dodawania danych AI do index.json
    """

    def __init__(self):
        self.matcher = SBERTFileMatcher()
        # Pobierz folder roboczy z konfiguracji
        self.work_directory = get_work_directory_from_config()
        if not self.work_directory:
            logger.warning("Brak folderu roboczego w konfiguracji")

    def load_existing_index(self, folder_path: str) -> Dict:
        """
        Åaduje istniejÄ…cy plik index.json
        """
        index_path = os.path.join(folder_path, "index.json")

        if os.path.exists(index_path):
            try:
                with open(index_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except json.JSONDecodeError as e:
                logger.error(f"BÅ‚Ä…d odczytu {index_path}: {e}")
                return {}
        else:
            logger.info(f"Brak pliku index.json w {folder_path}")
            return {}

    def save_index_with_ai_data(self, folder_path: str, index_data: Dict):
        """
        Zapisuje plik index.json z danymi AI
        """
        index_path = os.path.join(folder_path, "index.json")

        try:
            with open(index_path, "w", encoding="utf-8") as f:
                json.dump(index_data, f, indent=4, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Zapisano AI dane do: {index_path}")
        except Exception as e:
            logger.error(f"BÅ‚Ä…d zapisu {index_path}: {e}")

    def collect_files_in_folder(self, folder_path: str) -> Tuple[List[str], List[str]]:
        """
        Zbiera pliki archiwÃ³w i obrazÃ³w w folderze
        """
        archive_files = []
        image_files = []
        start_time = time.time()
        SCAN_TIMEOUT_SECONDS = 30  # Timeout dla skanowania pojedynczego folderu

        try:
            for entry in os.scandir(folder_path):
                if time.time() - start_time > SCAN_TIMEOUT_SECONDS:
                    logger.warning(f"â° Przekroczono limit czasu skanowania ({SCAN_TIMEOUT_SECONDS}s) w folderze {folder_path}")
                    break

                try:
                    if entry.is_file(follow_symlinks=False):
                        filename = entry.name.lower()

                        if filename == "index.json":
                            continue

                        if filename.endswith(IMAGE_EXTENSIONS):
                            image_files.append(entry.name)
                        else:
                            # Wszystkie inne pliki traktujemy jako archiwa/modele
                            archive_files.append(entry.name)
                except UnicodeEncodeError as e:
                    logger.error(f"BÅ‚Ä…d kodowania nazwy pliku w {folder_path}: {e}")
                    continue
                except OSError as e:
                    logger.error(f"BÅ‚Ä…d dostÄ™pu do pliku w {folder_path}: {e}")
                    continue

        except OSError as e:
            logger.error(f"BÅ‚Ä…d skanowania folderu {folder_path}: {e}")
        except Exception as e:
            logger.error(f"Nieoczekiwany bÅ‚Ä…d podczas skanowania {folder_path}: {e}")

        return archive_files, image_files

    def process_folder(self, folder_path: str, progress_callback=None) -> bool:
        """
        Przetwarza jeden folder - dodaje dane AI do index.json
        """
        logger.info(f"ğŸ” Przetwarzanie AI folderu: {folder_path}")

        if progress_callback:
            progress_callback(f"ğŸ” Przetwarzanie AI folderu: {folder_path}")

        if not os.path.isdir(folder_path):
            logger.error(f"âŒ ÅšcieÅ¼ka nie jest folderem: {folder_path}")
            return False

        # SprawdÅº czy istnieje index.json (folder musi byÄ‡ juÅ¼ przeskanowany)
        index_json_path = os.path.join(folder_path, "index.json")
        if not os.path.exists(index_json_path):
            logger.warning(f"âš ï¸ Brak index.json w folderze: {folder_path}")
            if progress_callback:
                progress_callback(f"âš ï¸ Brak index.json w folderze: {folder_path}")
            return False

        # Zbierz pliki
        archive_files, image_files = self.collect_files_in_folder(folder_path)

        if not archive_files and not image_files:
            logger.info(f"âš ï¸ Folder pusty (brak plikÃ³w do analizy AI): {folder_path}")
            if progress_callback:
                progress_callback(
                    f"âš ï¸ Folder pusty (brak plikÃ³w do analizy AI): {folder_path}"
                )
            return True

        logger.info(
            f"ğŸ“Š Znaleziono: {len(archive_files)} archiwÃ³w, {len(image_files)} obrazÃ³w"
        )

        # ZaÅ‚aduj istniejÄ…cy index.json
        index_data = self.load_existing_index(folder_path)

        # SprawdÅº czy AI juÅ¼ przetwarzaÅ‚o ten folder
        if "AI_processing_date" in index_data:
            logger.info(f"ğŸ”„ AktualizujÄ™ istniejÄ…ce dane AI dla: {folder_path}")
            if progress_callback:
                progress_callback(
                    f"ğŸ”„ AktualizujÄ™ istniejÄ…ce dane AI dla: {folder_path}"
                )
        else:
            logger.info(f"ğŸ†• Pierwsze przetwarzanie AI dla: {folder_path}")
            if progress_callback:
                progress_callback(f"ğŸ†• Pierwsze przetwarzanie AI dla: {folder_path}")

        # JeÅ›li nie ma podstawowej struktury, utwÃ³rz jÄ…
        if "folder_info" not in index_data:
            index_data["folder_info"] = {
                "path": os.path.abspath(folder_path),
                "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        # Dodaj sekcjÄ™ AI
        ai_data = {
            "AI_processing_date": datetime.now().isoformat(),
            "AI_model_info": {
                "name": "sentence-transformers/all-MiniLM-L6-v2",
                "type": "SBERT",
                "version": "1.0",
            },
            "AI_file_analysis": {
                "total_archive_files": len(archive_files),
                "total_image_files": len(image_files),
                "archive_files": archive_files,
                "image_files": image_files,
            },
        }

        # ZnajdÅº dopasowania AI
        if archive_files and image_files:
            logger.info("ğŸ¤– Uruchamiam analizÄ™ AI...")
            if progress_callback:
                progress_callback("ğŸ¤– Uruchamiam analizÄ™ AI...")

            start_time = time.time()

            matches = self.matcher.find_best_matches(archive_files, image_files)

            ai_time = time.time() - start_time
            logger.info(f"â±ï¸ Analiza AI zakoÅ„czona w {ai_time:.2f}s")
            if progress_callback:
                progress_callback(f"â±ï¸ Analiza AI zakoÅ„czona w {ai_time:.2f}s")

            ai_data["AI_matches"] = matches
            ai_data["AI_statistics"] = {
                "total_possible_pairs": len(archive_files) * len(image_files),
                "found_matches": len(matches),
                "match_rate": len(matches) / len(archive_files) if archive_files else 0,
                "processing_time_seconds": ai_time,
                "high_confidence_matches": len(
                    [m for m in matches if m["confidence_level"] == "HIGH"]
                ),
                "medium_confidence_matches": len(
                    [m for m in matches if m["confidence_level"] == "MEDIUM"]
                ),
            }

            # Dodaj szczegÃ³Å‚owe analizy dla najlepszych dopasowaÅ„
            detailed_analyses = []
            for match in matches[:3]:  # Tylko 3 najlepsze dla oszczÄ™dnoÅ›ci miejsca
                analysis = self.matcher.analyze_similarity_details(
                    match["archive_file"], match["image_file"]
                )
                detailed_analyses.append(analysis)

            ai_data["AI_detailed_analysis_samples"] = detailed_analyses

            if progress_callback:
                progress_callback(f"âœ… Znaleziono {len(matches)} dopasowaÅ„ AI")

        else:
            ai_data["AI_matches"] = []
            ai_data["AI_statistics"] = {
                "total_possible_pairs": 0,
                "found_matches": 0,
                "match_rate": 0,
                "reason": "Brak plikÃ³w archiwÃ³w lub obrazÃ³w do dopasowania",
            }

        # Dodaj dane AI do index_data
        for key, value in ai_data.items():
            index_data[key] = value

        # Zapisz plik
        self.save_index_with_ai_data(folder_path, index_data)

        return True

    def process_folder_recursive(self, root_folder_path: str, progress_callback=None):
        """
        Przetwarza folder rekurencyjnie (Å‚Ä…cznie z podfolderami)
        """
        logger.info(f"ğŸš€ Rozpoczynam rekurencyjne przetwarzanie AI: {root_folder_path}")

        if progress_callback:
            progress_callback(
                f"ğŸš€ Rozpoczynam rekurencyjne przetwarzanie AI: {root_folder_path}"
            )

        processed_folders = 0
        error_folders = 0

        try:
            for root, dirs, files in os.walk(root_folder_path, onerror=lambda e: logger.error(f"BÅ‚Ä…d podczas chodzenia po katalogu: {e}")):
                try:
                    # PomiÅ„ linki symboliczne
                    if os.path.islink(root):
                        continue

                    # SprawdÅº czy folder zawiera index.json (zostaÅ‚ juÅ¼ przeskanowany)
                    index_json_path = os.path.join(root, "index.json")
                    if not os.path.exists(index_json_path):
                        logger.debug(f"â­ï¸ Pomijam folder bez index.json: {root}")
                        continue

                    logger.info(f"ğŸ“ Przetwarzam AI dla folderu: {root}")
                    if progress_callback:
                        progress_callback(f"ğŸ“ Przetwarzam AI dla folderu: {root}")

                    if self.process_folder(root, progress_callback):
                        processed_folders += 1
                    else:
                        error_folders += 1

                except UnicodeEncodeError as e:
                    logger.error(f"BÅ‚Ä…d kodowania w folderze {root}: {e}")
                    error_folders += 1
                    continue
                except OSError as e:
                    logger.error(f"BÅ‚Ä…d systemowy w folderze {root}: {e}")
                    error_folders += 1
                    continue
                except Exception as e:
                    logger.error(f"Nieoczekiwany bÅ‚Ä…d w folderze {root}: {e}")
                    error_folders += 1
                    continue

        except Exception as e:
            logger.error(f"Krytyczny bÅ‚Ä…d podczas rekurencyjnego przetwarzania: {e}")
            if progress_callback:
                progress_callback(f"âŒ Krytyczny bÅ‚Ä…d: {e}")

        success_msg = f"âœ… Przetwarzanie AI zakoÅ„czone: {processed_folders} folderÃ³w OK, {error_folders} bÅ‚Ä™dÃ³w"
        logger.info(success_msg)
        if progress_callback:
            progress_callback(success_msg)

        return processed_folders > 0

    def start_ai_processing(self, progress_callback=None):
        """Rozpoczyna przetwarzanie AI od folderu roboczego z konfiguracji"""
        if not self.work_directory:
            logger.error("Brak folderu roboczego w konfiguracji")
            if progress_callback:
                progress_callback("âŒ Brak folderu roboczego w konfiguracji")
            return False

        if not os.path.isdir(self.work_directory):
            logger.error(f"Folder roboczy nie istnieje: {self.work_directory}")
            if progress_callback:
                progress_callback(
                    f"âŒ Folder roboczy nie istnieje: {self.work_directory}"
                )
            return False

        logger.info(f"ğŸ¤– Rozpoczynam przetwarzanie AI dla: {self.work_directory}")
        if progress_callback:
            progress_callback(
                f"ğŸ¤– Rozpoczynam przetwarzanie AI dla: {self.work_directory}"
            )

        return self.process_folder_recursive(self.work_directory, progress_callback)


def main():
    """
    Funkcja gÅ‚Ã³wna - automatycznie pobiera folder roboczy z konfiguracji
    """
    print("ğŸ¤– AI SBERT File Matcher - Automatyczne przetwarzanie")
    print("=" * 60)

    # UtwÃ³rz procesor i sprawdÅº konfiguracjÄ™
    processor = AIFolderProcessor()

    if not processor.work_directory:
        print("âŒ Brak folderu roboczego w konfiguracji!")
        print("ğŸ’¡ Uruchom najpierw gÅ‚Ã³wnÄ… aplikacjÄ™ i ustaw folder roboczy.")
        return

    print(f"ğŸ“ Folder roboczy z konfiguracji: {processor.work_directory}")

    if not os.path.exists(processor.work_directory):
        print(f"âŒ Folder roboczy nie istnieje: {processor.work_directory}")
        return

    # Zapytaj o tryb przetwarzania
    print("\nğŸ”„ Tryby przetwarzania:")
    print("1. Automatyczne (caÅ‚y folder roboczy)")
    print("2. Konkretny folder")
    print("3. WyjÅ›cie")

    choice = input("\nWybierz opcjÄ™ (1-3): ").strip()

    if choice == "1":
        # Automatyczne przetwarzanie caÅ‚ego folderu roboczego
        print(f"\nğŸš€ Rozpoczynam automatyczne przetwarzanie AI...")
        processor.start_ai_processing(print)

    elif choice == "2":
        # Konkretny folder
        test_folder = input("Podaj Å›cieÅ¼kÄ™ do konkretnego folderu: ").strip()
        if not test_folder:
            print("âŒ Nie podano Å›cieÅ¼ki")
            return

        if not os.path.exists(test_folder):
            print(f"âŒ Folder nie istnieje: {test_folder}")
            return

        print(f"ğŸ” Przetwarzam konkretny folder: {test_folder}")
        processor.process_folder_recursive(test_folder, print)

    elif choice == "3":
        print("ğŸ‘‹ Do widzenia!")
        return
    else:
        print("âŒ NieprawidÅ‚owy wybÃ³r")
        return

    print("\nğŸ‰ Przetwarzanie AI zakoÅ„czone! SprawdÅº pliki index.json w folderach.")
    print("ğŸ” Wyszukaj klucze zaczynajÄ…ce siÄ™ od 'AI_' aby zobaczyÄ‡ wyniki.")


def generate_ai_only_gallery_data(folder_path: str) -> Dict:
    """
    Generuje dane galerii zawierajÄ…ce tylko dopasowania AI
    """
    index_path = os.path.join(folder_path, "index.json")

    if not os.path.exists(index_path):
        logger.warning(f"Brak index.json w {folder_path}")
        return {}

    try:
        with open(index_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        logger.error(f"BÅ‚Ä…d odczytu index.json w {folder_path}: {e}")
        return {}

    # SprawdÅº czy sÄ… dane AI
    if "AI_matches" not in data:
        logger.debug(f"Brak danych AI_matches w {folder_path}")
        return {}

    ai_matches = data.get("AI_matches", [])
    if not ai_matches:
        logger.debug(f"Pusta lista AI_matches w {folder_path}")
        return {}

    logger.info(
        f"GenerujÄ™ dane galerii AI dla {folder_path} z {len(ai_matches)} dopasowaniami"
    )

    # UtwÃ³rz strukturÄ™ danych tylko z dopasowaniami AI
    ai_gallery_data = {
        "folder_info": data.get("folder_info", {}).copy(),
        "files_with_previews": [],
        "files_without_previews": [],
        "other_images": [],
        "ai_info": {
            "processing_date": data.get("AI_processing_date"),
            "model_info": data.get("AI_model_info", {}),
            "statistics": data.get("AI_statistics", {}),
            "total_matches": len(ai_matches),
        },
    }

    # Dodaj informacje AI do folder_info
    ai_gallery_data["folder_info"]["gallery_type"] = "AI_POWERED"
    ai_gallery_data["folder_info"]["ai_matches_count"] = len(ai_matches)
    ai_gallery_data["folder_info"]["ai_scan_date"] = data.get("AI_processing_date")

    # Konwertuj dopasowania AI na format galerii
    processed_matches = 0
    for match in ai_matches:
        archive_file = match.get("archive_file")
        image_file = match.get("image_file")

        if not archive_file or not image_file:
            logger.warning(f"NiepeÅ‚ne dopasowanie AI: {match}")
            continue

        # ZnajdÅº peÅ‚ne Å›cieÅ¼ki plikÃ³w
        archive_path = os.path.join(folder_path, archive_file)
        image_path = os.path.join(folder_path, image_file)

        if not os.path.exists(archive_path):
            logger.warning(f"Plik archiwum nie istnieje: {archive_path}")
            continue

        if not os.path.exists(image_path):
            logger.warning(f"Plik obrazu nie istnieje: {image_path}")
            continue

        try:
            archive_size = os.path.getsize(archive_path)
        except OSError:
            archive_size = 0

        file_info = {
            "name": archive_file,
            "path_absolute": os.path.abspath(archive_path),
            "size_bytes": archive_size,
            "size_readable": get_file_size_readable_ai(archive_size),
            "preview_found": True,
            "preview_name": image_file,
            "preview_path_absolute": os.path.abspath(image_path),
            "preview_relative_path": f"file:///{os.path.abspath(image_path).replace(os.sep, '/')}",
            "archive_link": f"file:///{os.path.abspath(archive_path).replace(os.sep, '/')}",
            "ai_match": True,
            "ai_confidence": match.get("confidence_level", "UNKNOWN"),
            "ai_similarity_score": match.get("similarity_score", 0.0),
            "ai_matching_method": match.get("matching_method", "SBERT"),
            "ai_timestamp": match.get("timestamp"),
        }

        # Dodaj kolor archiwum jeÅ›li dostÄ™pny
        file_ext = os.path.splitext(archive_file)[1].lower()
        try:
            import config_manager

            file_info["archive_color"] = config_manager.get_archive_color(file_ext)
        except:
            file_info["archive_color"] = "#6c757d"

        ai_gallery_data["files_with_previews"].append(file_info)
        processed_matches += 1

    logger.info(
        f"Przetworzone {processed_matches}/{len(ai_matches)} dopasowaÅ„ AI dla {folder_path}"
    )

    return ai_gallery_data


def get_file_size_readable_ai(size_bytes):
    """Funkcja pomocnicza do konwersji rozmiaru pliku dla AI"""
    if size_bytes == 0:
        return "0 B"
    size_name = ("B", "KB", "MB", "GB", "TB")
    i = 0
    size_float = float(size_bytes)
    while size_float >= 1024 and i < len(size_name) - 1:
        size_float /= 1024.0
        i += 1
    return f"{size_float:.2f} {size_name[i]}"


if __name__ == "__main__":
    main()
